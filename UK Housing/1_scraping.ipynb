{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was created by Julia Drygalska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "### For this part, I creaed a bucket in my AWS environment and uploaded the dataset inside of it for later use. Then, to avoid memory issues i created a subset of 30% of all dataset and saved it locally. Next step is explained in the \"cleaning\" notebook, since I have to clean it before I can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load CSV from S3\n",
    "\n",
    "# Define the S3 URI for the original CSV dataset\n",
    "s3_csv_uri = 's3://bucketjdrygalska/price_paid_records.csv'\n",
    "\n",
    "# Load the CSV file from S3\n",
    "print(\"Loading CSV from S3...\")\n",
    "df = pd.read_csv(s3_csv_uri)\n",
    "print(\"CSV loaded successfully!\")\n",
    "\n",
    "# Remove the first 70% of rows\n",
    "percent_to_remove = 70\n",
    "rows_to_remove = int(len(df) * (percent_to_remove / 100))\n",
    "df_subset = df.iloc[rows_to_remove:]\n",
    "print(f\"Removed the first {percent_to_remove}% of rows. Remaining rows: {len(df_subset)}\")\n",
    "\n",
    "# Define the local path to save files inside the UK Housing directory\n",
    "local_path = './datasets/'\n",
    "\n",
    "# Ensure the local path exists\n",
    "if not os.path.exists(local_path):\n",
    "    os.makedirs(local_path)\n",
    "\n",
    "# Save the subset DataFrame to CSV format (locally, inside UK Housing directory)\n",
    "local_csv_path = local_path + 'price_paid_records_subset.csv'\n",
    "print(\"Saving subset dataset as CSV locally...\")\n",
    "df_subset.to_csv(local_csv_path, index=False)\n",
    "print(\"Subset dataset saved as CSV locally successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After I have my subset cleaned, I split it into 2 different csv files: one for training, one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset by Date for Training and Testing\n",
    "\n",
    "# Load cleaned dataset\n",
    "df = pd.read_csv('./datasets/price_paid_records_cleaned.csv')\n",
    "\n",
    "# Convert 'date_of_transfer' to datetime for filtering purposes\n",
    "df['date_of_transfer'] = pd.to_datetime(df['date_of_transfer'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid 'date_of_transfer'\n",
    "df = df.dropna(subset=['date_of_transfer'])\n",
    "\n",
    "# Split the dataset into training and testing sets based on 'date_of_transfer'\n",
    "train_data = df[df['date_of_transfer'].dt.year <= 2015].copy()\n",
    "test_data = df[df['date_of_transfer'].dt.year > 2015].copy()\n",
    "\n",
    "# Save the training and testing datasets\n",
    "train_data_path = 'datasets/train_data.csv'\n",
    "train_data.to_csv(train_data_path, index=False)\n",
    "\n",
    "test_data_path = 'datasets/test_data.csv'\n",
    "test_data.to_csv(test_data_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
